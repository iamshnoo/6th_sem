{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Data source available.\nShape of original data : 150, 4\nNormalizing the data to to range [0,1] now...\nShape of the normalized data : 150, 4\nPreparing dissimilarity matrix...\nShape of the Dissimilarity Matrix : 150, 150\nCalculating average dissimilarity values for each object...\nPrimary clustering...\n150 primary clusters formed.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 14, 14\nMerging clusters having maximum similarity...\n14 clusters found.\nNone\n0.9789473684210527 is the maximum similarity value and clusters 13, 10 are the most similar.\n13 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 13, 13\nMerging clusters having maximum similarity...\n13 clusters found.\nNone\n0.968421052631579 is the maximum similarity value and clusters 11, 9 are the most similar.\n12 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 12, 12\nMerging clusters having maximum similarity...\n12 clusters found.\nNone\n0.9587628865979382 is the maximum similarity value and clusters 11, 9 are the most similar.\n11 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 11, 11\nMerging clusters having maximum similarity...\n11 clusters found.\nNone\n0.9552238805970149 is the maximum similarity value and clusters 6, 5 are the most similar.\n10 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 10, 10\nMerging clusters having maximum similarity...\n10 clusters found.\nNone\n0.9506172839506173 is the maximum similarity value and clusters 7, 6 are the most similar.\n9 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 9, 9\nMerging clusters having maximum similarity...\n9 clusters found.\nNone\n0.935064935064935 is the maximum similarity value and clusters 2, 1 are the most similar.\n8 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 8, 8\nMerging clusters having maximum similarity...\n8 clusters found.\nNone\n0.92 is the maximum similarity value and clusters 7, 6 are the most similar.\n7 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 7, 7\nMerging clusters having maximum similarity...\n7 clusters found.\nNone\n0.9186046511627907 is the maximum similarity value and clusters 5, 2 are the most similar.\n6 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 6, 6\nMerging clusters having maximum similarity...\n6 clusters found.\nNone\n0.8045977011494253 is the maximum similarity value and clusters 4, 2 are the most similar.\n5 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 5, 5\nMerging clusters having maximum similarity...\n5 clusters found.\nNone\n0.6923076923076923 is the maximum similarity value and clusters 1, 0 are the most similar.\n4 clusters left after this iteration.\n---------------------------------------------------------------\nRemoving clusters which are a subset of other clusters...\nCreating similarity matrix...\nShape of the Similarity Matrix : 4, 4\nMerging clusters having maximum similarity...\n4 clusters found.\nNone\n0.6559139784946236 is the maximum similarity value and clusters 2, 1 are the most similar.\n3 clusters left after this iteration.\n---------------------------------------------------------------\n78, 93, 100 are the sizes of the three final clusters.\n\nCluster 1 : \n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 55, 57, 59, 60, 62, 64, 67, 69, 71, 79, 80, 81, 82, 84, 85, 87, 88, 89, 90, 92, 93, 94, 95, 96, 98, 99, 106]\n\n\nCluster 2 : \n\n[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 20, 23, 24, 25, 26, 29, 30, 31, 34, 35, 38, 41, 42, 43, 45, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 103, 106, 111, 113, 119, 121, 123, 126, 127, 133, 134, 138, 142, 146, 149]\n\n\nCluster 3 : \n\n[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]\n\n\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "class Iris:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.m = 150\n",
    "        self.n = 4\n",
    "        self.k = 3\n",
    "\n",
    "    def getK(self):\n",
    "        return self.k\n",
    "\n",
    "    def getData(self):\n",
    "\n",
    "        try:\n",
    "            data = np.genfromtxt('iris.csv', delimiter=',')\n",
    "            print(\"Data source available.\")\n",
    "        except IOError:\n",
    "            print(\"Missing dataset! Run:\")\n",
    "            print(\n",
    "                \"wget http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n",
    "            exit(0)\n",
    "        print(\"Shape of original data : \" + str(len(data)) +\n",
    "              \", \" + str(len(data[0])))\n",
    "        return data\n",
    "\n",
    "    def normalizeData(self, data):\n",
    "        print(\"Normalizing the data to to range [0,1] now...\")\n",
    "        low, high = np.amin(data, axis=0), np.amax(data, axis=0)\n",
    "        for j in range(self.n):\n",
    "            minimum, maximum = low[j], high[j]\n",
    "            for i in range(self.m):\n",
    "                data[i][j] = (data[i][j] - minimum)/(maximum - minimum)\n",
    "        print(\"Shape of the normalized data : \" +\n",
    "              str(len(data)) + \", \" + str(len(data[0])))\n",
    "        return data\n",
    "\n",
    "    def getDissimilarityMatrix(self, data):\n",
    "        print(\"Preparing dissimilarity matrix...\")\n",
    "        mat = np.zeros(shape=(self.m, self.m))\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                # mat[i, j]= np.linalg.norm(data[i] - data[j]) --> this is slow\n",
    "                mat[i, j] = ((data[i][0] - data[j][0])**2 + (data[i][1] - data[j][1]\n",
    "                                                             )**2 + (data[i][2] - data[j][2])**2 + (data[i][3] - data[j][3])**2)**0.5\n",
    "        print(\"Shape of the Dissimilarity Matrix : \" +\n",
    "              str(len(mat)) + \", \" + str(len(mat[0])))\n",
    "        return mat\n",
    "\n",
    "    def cluster(self, mat):\n",
    "        print(\"Calculating average dissimilarity values for each object...\")\n",
    "        avg = np.mean(mat, axis=0)\n",
    "        print(\"Primary clustering...\")\n",
    "        clusters = []\n",
    "        for i in range(self.m):\n",
    "            cluster = []\n",
    "            for j in range(self.m):\n",
    "                if(mat[i][j] < avg[i]):\n",
    "                    cluster.append(j)\n",
    "            clusters.append(cluster)\n",
    "        print(str(len(clusters)) + \" primary clusters formed.\")\n",
    "        return clusters\n",
    "\n",
    "    def removeSubsetClusters(self, clusters):\n",
    "        print(\"Removing clusters which are a subset of other clusters...\")\n",
    "        P = len(clusters)\n",
    "        i = 0\n",
    "        while i < P:\n",
    "            j = 0\n",
    "            while j < P:\n",
    "                if i != j:\n",
    "                    if (set(clusters[i]).issubset(set(clusters[j]))):\n",
    "                        clusters.remove(clusters[i])\n",
    "                        P -= 1\n",
    "                        i -= 1\n",
    "                        break\n",
    "                j += 1\n",
    "            i += 1\n",
    "        return clusters\n",
    "\n",
    "    def getSimilarityMatrix(self, clusters):\n",
    "        print(\"Creating similarity matrix...\")\n",
    "        p = len(clusters)\n",
    "        sim = np.zeros(shape=(p, p))\n",
    "        for i in range(p):\n",
    "            for j in range(p):\n",
    "                sim[i, j] = len(list(set(clusters[i]) & set(clusters[j]))) / \\\n",
    "                    len(list(set(clusters[i]) | set(clusters[j])))\n",
    "        print(\"Shape of the Similarity Matrix : \" +\n",
    "              str(len(sim)) + \", \" + str(len(sim[0])))\n",
    "        return sim\n",
    "\n",
    "    def mergeMaxSimilarityClusters(self, sim, clusters):\n",
    "        print(\"Merging clusters having maximum similarity...\")\n",
    "        print(print(str(len(clusters)) + \" clusters found.\"))\n",
    "        val = 0\n",
    "        idx_k = -1\n",
    "        idx_l = -1\n",
    "        p = len(clusters)\n",
    "        for i in range(p):\n",
    "            for j in range(p):\n",
    "                if(i != j):\n",
    "                    if(sim[i, j] >= val):\n",
    "                        val = sim[i, j]\n",
    "                        idx_k = i\n",
    "                        idx_l = j\n",
    "\n",
    "        clusters[idx_k] = list(set(clusters[idx_k]) | set(clusters[idx_l]))\n",
    "        clusters.remove(clusters[idx_l])\n",
    "\n",
    "        print(str(val) + \" is the maximum similarity value and clusters \" +\n",
    "              str(idx_k) + \", \" + str(idx_l) + \" are the most similar.\")\n",
    "        print(str(len(clusters)) + \" clusters left after this iteration.\")\n",
    "        return clusters\n",
    "\n",
    "\n",
    "\n",
    "model = Iris()\n",
    "data = Iris.getData(model)\n",
    "data = Iris.normalizeData(model, data)\n",
    "mat = Iris.getDissimilarityMatrix(model, data)\n",
    "clusters = Iris.cluster(model, mat)\n",
    "cluster_original = clusters\n",
    "print(\"---------------------------------------------------------------\")\n",
    "while(len(clusters) != Iris.getK(model)):\n",
    "    clusters = Iris.removeSubsetClusters(model, clusters)\n",
    "    sim = Iris.getSimilarityMatrix(model, clusters)\n",
    "    clusters = Iris.mergeMaxSimilarityClusters(model, sim, clusters)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "print(str(len(clusters[0])) + \", \" + str(len(clusters[1])) + \", \" +\n",
    "        str(len(clusters[2])) + \" are the sizes of the three final clusters.\\n\")\n",
    "print(\"Cluster 1 : \\n\")\n",
    "print(clusters[0])\n",
    "print(\"\\n\")\n",
    "print(\"Cluster 2 : \\n\")\n",
    "print(clusters[1])\n",
    "print(\"\\n\")\n",
    "print(\"Cluster 3 : \\n\")\n",
    "print(clusters[2])\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "44, 46, 60 are the sizes of the three final clusters.\n\nCluster 1 : \n\n[0, 4, 5, 10, 14, 15, 16, 17, 18, 19, 21, 22, 27, 28, 32, 33, 36, 37, 39, 40, 44, 46, 48, 67, 69, 71, 79, 80, 81, 82, 84, 85, 87, 88, 89, 90, 92, 93, 94, 95, 96, 98, 99, 106]\n\n\nCluster 2 : \n\n[1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 20, 23, 24, 25, 26, 29, 30, 31, 34, 35, 38, 41, 42, 43, 45, 47, 49, 83, 86, 91, 97, 101, 103, 111, 113, 119, 121, 123, 126, 127, 133, 134, 138, 142, 146, 149]\n\n\nCluster 3 : \n\n[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 70, 72, 73, 74, 75, 76, 77, 78, 100, 102, 104, 105, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 120, 122, 124, 125, 128, 129, 130, 131, 132, 135, 136, 137, 139, 140, 141, 143, 144, 145, 147, 148]\n\n\nC:\\Users\\Tutun\\Miniconda3\\envs\\dataMining\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: divide by zero encountered in log\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def notkMeansClustering(clusters):\n",
    "    mean = []\n",
    "    for i in range(3):\n",
    "        sum = 0\n",
    "        for j in range(len(clusters[i])):\n",
    "            sum += clusters[i][j]\n",
    "        mean.append(sum/len(clusters[i]))\n",
    "\n",
    "    locations = [[] for _ in range(150)]\n",
    "    for i in range(150):\n",
    "        for j in range(3):\n",
    "            for k in range(len(clusters[j])):\n",
    "                if (clusters[j][k] == i):\n",
    "                    locations[i].append(j)\n",
    "                    break\n",
    "\n",
    "    dummy = [[] for _ in range(3)]\n",
    "    for i in range(150):\n",
    "        obj = i\n",
    "        max_similarity = 0 # find the maximum similarity across all the values of locations[i] for current obj\n",
    "        max_similarity_idx = -1 # find the cluster to which the object belongs the most\n",
    "        for j in range(len(locations[i])):\n",
    "            location = locations[i][j] # location is either 0 or 1 or 2\n",
    "            for k in range(len(clusters[location])):\n",
    "                # find the current object in clusters[location]\n",
    "                if (clusters[location][k] == obj):\n",
    "                    # then compare the similarity of clusters[location][k] with mean of clusters[location]\n",
    "                    num1 = clusters[location][k]\n",
    "                    num2 = mean[location]\n",
    "\n",
    "                    # Experimenting with possible similarity functions\n",
    "\n",
    "                    #similarity = min(num1, num2)/ max(num1, num2) # 51, 31, 68\n",
    "                    #similarity = abs(np.log(min(num1, num2))/np.log(max(num1, num2))) # 50, 32, 68\n",
    "                    #similarity = abs((np.log(num1))**2-(np.log(num2))**2)**0.5 # 43, 46, 61\n",
    "                    #similarity = ((np.log(num1))**2-(np.log(num2))**2)**0.5 # 34, 27, 89\n",
    "                    #similarity = abs(((np.log(num1))**2-(np.log(num2))**2)**0.5) # 34, 27, 89\n",
    "                    similarity = abs(np.log(num1)-np.log(num2)) # 44, 46, 60\n",
    "                    #similarity = (np.log(num1)-np.log(num2))**2 # 44, 46, 60\n",
    "                    #similarity = abs(np.log(num1)/np.log(num2)) # 77, 38, 35\n",
    "                    #similarity = abs(num1-num2) # 41, 45, 64\n",
    "                    #similarity = (num1 - num2)**2 # 41, 45, 64\n",
    "                    #similarity = np.dot(num1, num2)/(np.linalg.norm(num1)*np.linalg.norm(num2)) # 77, 38, 35\n",
    "\n",
    "                    # get the maximum similarity value for that object across all values in locations[i]\n",
    "                    # the object will get appended to dummy[max_similarity_idx]\n",
    "                    if (similarity >= max_similarity):\n",
    "                        max_similarity = similarity\n",
    "                        max_similarity_idx = location\n",
    "                    break\n",
    "        dummy[max_similarity_idx].append(i) # append the current object to the cluster with which it is most similar\n",
    "\n",
    "    clusters = dummy\n",
    "    return clusters\n",
    "\n",
    "clusters = [\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 53, 55, 57, 59, 60, 62, 64, 67, 69, 71, 79, 80, 81, 82, 84, 85, 87, 88, 89, 90, 92, 93, 94, 95, 96, 98, 99, 106],\n",
    "    [1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 20, 23, 24, 25, 26, 29, 30, 31, 34, 35, 38, 41, 42, 43, 45, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 103, 106, 111, 113, 119, 121, 123, 126, 127, 133, 134, 138, 142, 146, 149],\n",
    "    [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]\n",
    "]\n",
    "\n",
    "clusters = notkMeansClustering(clusters)\n",
    "print(str(len(clusters[0])) + \", \" + str(len(clusters[1])) + \", \" + str(len(clusters[2])) + \" are the sizes of the three final clusters.\\n\")\n",
    "print(\"Cluster 1 : \\n\")\n",
    "print(clusters[0])\n",
    "print(\"\\n\")\n",
    "print(\"Cluster 2 : \\n\")\n",
    "print(clusters[1])\n",
    "print(\"\\n\")\n",
    "print(\"Cluster 3 : \\n\")\n",
    "print(clusters[2])\n",
    "print(\"\\n\")\n",
    "\n",
    "def plot(clusters):\n",
    "    for i in range(3):\n",
    "        cluster = clusters[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Removing clusters which are a subset of other clusters...\n[[1, 2, 3, 4, 5]]\n"
    }
   ],
   "source": [
    "def removeSubsetClusters(clusters):\n",
    "    print(\"Removing clusters which are a subset of other clusters...\")\n",
    "    P = len(clusters)\n",
    "    i = 0\n",
    "    while i < P:\n",
    "        j = 0\n",
    "        while j < P:\n",
    "            if i != j:\n",
    "                if (set(clusters[i]).issubset(set(clusters[j]))):\n",
    "                    clusters.remove(clusters[i])\n",
    "                    P -= 1\n",
    "                    i -= 1\n",
    "                    break\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return clusters\n",
    "\n",
    "clusters = [\n",
    "\t\t\t[1,2,3,4,5],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1,2,4],\n",
    "\t\t\t[1,2],\n",
    "\t\t\t[4,5],\n",
    "\t\t\t[4]\n",
    "\t\t]\n",
    "p = removeSubsetClusters(clusters)\n",
    "print(p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "150 primary clusters formed.\n\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('iris.csv', delimiter=',')\n",
    "m = 150\n",
    "n = 4\n",
    "k = 3\n",
    "\n",
    "low = np.amin(data, axis=0)\n",
    "high = np.amax(data, axis=0)\n",
    "for j in range(n):\n",
    "    minimum = low[j]\n",
    "    maximum = high[j]\n",
    "    for i in range(m):\n",
    "        data[i][j] = (data[i][j] - minimum)/(maximum - minimum)\n",
    "\n",
    "mat = np.zeros(shape=(m, m))\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        mat[i, j] = ((data[i][0] - data[j][0])**2 + (data[i][1] - data[j][1]\n",
    "                                                        )**2 + (data[i][2] - data[j][2])**2 + (data[i][3] - data[j][3])**2)\n",
    "        mat[i, j] = mat[i, j]**0.5\n",
    "\n",
    "avg = np.zeros(shape=(m))\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        avg[i] += mat[i][j]\n",
    "    avg[i] = avg[i] / m\n",
    "\n",
    "clusters = []\n",
    "for i in range(m):\n",
    "    cluster = []\n",
    "    for j in range(m):\n",
    "        if(mat[i][j] < avg[j]):\n",
    "            cluster.append(i)\n",
    "    clusters.append(cluster)\n",
    "clusters = np.asarray(clusters)\n",
    "clusters = np.unique(clusters)\n",
    "print(str(len(clusters)) + \" primary clusters formed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n 144 145 146 147 148 149]\n150\n5504\n"
    }
   ],
   "source": [
    "idx = []\n",
    "count = 0\n",
    "x = len(clusters)\n",
    "for i in range(x):\n",
    "    for j in range(x):\n",
    "        if(i != j):\n",
    "            if(set(clusters[j]).issubset(set(clusters[i]))):\n",
    "                count+=1\n",
    "                idx.append(j)\n",
    "idx = np.unique(idx)\n",
    "idx = np.asarray(idx)\n",
    "print(idx)\n",
    "print(len(idx))\n",
    "print(count)\n",
    "#if(idx.size > 0):\n",
    "#    clusters_copy = np.delete(clusters, idx, 0)\n",
    "#print(str(len(clusters_copy)) + \" clusters left after reduction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Iris:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.m = 150\n",
    "        self.n = 4\n",
    "        self.k = 3\n",
    "    \n",
    "    def getK(self):\n",
    "        return self.k\n",
    "\n",
    "    def getData():\n",
    "\n",
    "        try:\n",
    "            data = np.genfromtxt('iris.csv', delimiter=',')\n",
    "            print(\"Data prepared.\\n\") \n",
    "            #print(\"Original Data : \\n\")\n",
    "            #print(data)\n",
    "            #print(\"\\n\")\n",
    "        except IOError:\n",
    "            print(\"Missing dataset! Run:\")\n",
    "            print(\n",
    "                \"wget http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n",
    "            data = -1\n",
    "        #print(data)\n",
    "        #print(\"\\n\")\n",
    "        print(\"Shape of the data : \" + str(len(data)) + \"\\n\")\n",
    "        return data\n",
    "\n",
    "    def normalizeData(self, data):\n",
    "        print(\"Normalizing the data now...\\n\")\n",
    "        low = np.amin(data, axis=0)\n",
    "        high = np.amax(data, axis=0)\n",
    "        for j in range(self.n):\n",
    "            minimum = low[j]\n",
    "            maximum = high[j]\n",
    "            for i in range(self.m):\n",
    "                data[i][j] = (data[i][j] - minimum)/(maximum - minimum)\n",
    "        #print(\"Normalized Data : \\n\")\n",
    "        #print(data)\n",
    "        #print(\"\\n\")\n",
    "        print(\"Shape of the normalized data : \" + str(len(data)) + \"\\n\")\n",
    "        return data\n",
    "\n",
    "    def getDissimilarityMatrix(self, data):\n",
    "        print(\"Preparing dissimilarity matrix...\\n\")\n",
    "        mat = np.zeros(shape=(self.m, self.m))\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                mat[i, j] = ((data[i][0] - data[j][0])**2 + (data[i][1] - data[j][1]\n",
    "                                                             )**2 + (data[i][2] - data[j][2])**2 + (data[i][3] - data[j][3])**2)\n",
    "                mat[i, j] = mat[i, j]**0.5\n",
    "        #print(\"Dissimilarity Matrix : \\n\")\n",
    "        #print(mat)\n",
    "        #print(\"\\n\")\n",
    "        print(\"Shape of the Dissimilarity Matrix : \" + str(len(mat)) + \"\\n\")\n",
    "        return mat\n",
    "\n",
    "    def cluster(self, mat):\n",
    "        print(\"Calculating average dissimilarity values for each object...\\n\")\n",
    "        avg = np.zeros(shape=(self.m))\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                avg[i] += mat[i][j]\n",
    "            avg[i] = avg[i] / self.m\n",
    "\n",
    "        print(\"Primary clustering...\\n\")\n",
    "        clusters = []\n",
    "        for i in range(self.m):\n",
    "            cluster = []\n",
    "            for j in range(self.m):\n",
    "                if(mat[i][j] < avg[j]):\n",
    "                    cluster.append(j)\n",
    "            clusters.append(cluster)\n",
    "        clusters = np.asarray(clusters)\n",
    "        print(str(len(clusters)) + \" primary clusters formed.\\n\")\n",
    "        return clusters\n",
    "    '''\n",
    "    def removeSubsetClusters(self, clusters):\n",
    "        print(\"Removing clusters which are a subset of other clusters...\\n\")\n",
    "        idx = []\n",
    "        x = len(clusters)\n",
    "        for i in range(x):\n",
    "            for j in range(x):\n",
    "                if(i != j):\n",
    "                    if(len(np.setdiff1d(clusters[i], clusters[j])) == 0):\n",
    "                        for k in range(len(clusters[j])):\n",
    "                            clusters[j][k] = 0\n",
    "                        idx.append(j)\n",
    "        idx = np.unique(idx)\n",
    "        clusters = np.delete(clusters, idx, 0)\n",
    "        print(str(len(clusters)) + \" clusters left after reduction.\\n\")\n",
    "        return clusters\n",
    "    '''\n",
    "    def removeSubsetClusters(self, clusters):\n",
    "        print(\"Removing clusters which are a subset of other clusters...\")\n",
    "        idx = []\n",
    "        x = len(clusters)\n",
    "        for i in range(x):\n",
    "            for j in range(x):\n",
    "                if(i != j):\n",
    "                    a = clusters[i]\n",
    "                    b = clusters[j]\n",
    "                    b = set(b)\n",
    "                    #a = a.tolist()\n",
    "                    ismember = b.issubset(a)\n",
    "                    if(ismember):\n",
    "                        b_id = np.where(ismember)[0]\n",
    "                        idx.append(b_id)\n",
    "        idx = np.unique(idx)\n",
    "        if(idx.size > 0):\n",
    "            clusters = np.delete(clusters, idx, 0)\n",
    "        print(str(len(clusters)) + \" clusters left after reduction.\")\n",
    "        return clusters\n",
    "\n",
    "    def getSimilarityMatrix(clusters):\n",
    "        print(\"Creating similarity matrix...\\n\")\n",
    "        p = len(clusters)\n",
    "        sim = np.zeros(shape=(p, p))\n",
    "        for i in range(p):\n",
    "            for j in range(p):\n",
    "                sim[i, j] = len(np.intersect1d(clusters[i], clusters[j])\n",
    "                                ) / len(np.union1d(clusters[i], clusters[j]))\n",
    "        print(\"Similarity Matrix :\\n\")\n",
    "        print(sim)\n",
    "        print(\"\\n\")\n",
    "        print(\"Shape of the Similarity Matrix : \" + str(len(sim)) + \"\\n\")\n",
    "        return sim\n",
    "\n",
    "    def mergeMaxSimilarityClusters(sim, clusters):\n",
    "        print(\"Merging clusters having maximum similarity...\\n\")\n",
    "        print(print(str(len(clusters)) + \" clusters found.\\n\"))\n",
    "        val = 0\n",
    "        idx_k = 0\n",
    "        idx_l = 0\n",
    "        p = len(clusters)\n",
    "        for i in range(p):\n",
    "            for j in range(p):\n",
    "                if(i != j):\n",
    "                    if(sim[i, j] > val):\n",
    "                        val = sim[i, j]\n",
    "                        idx_k = i\n",
    "                        idx_l = j\n",
    "\n",
    "        print(str(val) + \" is the maximum similarity value and clusters \" +\n",
    "              str(idx_k) + \", \" + str(idx_l) + \" are the most similar.\")\n",
    "        merged_clusters = np.union1d(clusters[idx_k], clusters[idx_l])\n",
    "        ids = [idx_k, idx_l]\n",
    "        clusters = np.delete(clusters, ids, 0)\n",
    "        dummy = [[] for _ in range(len(clusters))]\n",
    "        for i in range(len(clusters)):\n",
    "            for j in range(len(clusters[i])):\n",
    "                dummy[i].append(clusters[i][j])\n",
    "        dummy.append(merged_clusters)\n",
    "        clusters = dummy\n",
    "        print(str(len(clusters)) + \" clusters left now.\\n\")\n",
    "        return clusters\n",
    "\n",
    "model = Iris()\n",
    "data = Iris.getData()\n",
    "data = Iris.normalizeData(model,data)\n",
    "mat = Iris.getDissimilarityMatrix(model, data)\n",
    "clusters = Iris.cluster(model, mat)\n",
    "cluster_original = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(len(clusters)!=Iris.getK(model)):\n",
    "    clusters = Iris.removeSubsetClusters(model, clusters)\n",
    "    sim = Iris.getSimilarityMatrix(clusters)\n",
    "    clusters = Iris.mergeMaxSimilarityClusters(sim, clusters)\n",
    "print(str(len(clusters[0])) + \", \" + str(len(clusters[1])) +\", \" + str(len(clusters[2])) + \" are the sizes of the three clusters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [[1,2,3],[4,5,6],[7,8,9],[1,2,3]]\n",
    "a = np.asarray(a)\n",
    "b = [1,2,3]\n",
    "b = set(b)\n",
    "ismember = [b.issubset(row) for row in a.tolist()]\n",
    "idx = np.where(ismember)[0]\n",
    "idx = np.asarray(idx)\n",
    "idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "j = 57\n",
    "#print(str(np.setdiff1d(clusters[i], clusters[j])))\n",
    "a = set(clusters[i])\n",
    "b = set(clusters[j])\n",
    "print(a)\n",
    "print(b)\n",
    "print(b.issubset(a))"
   ]
  }
 ]
}